{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = 1000  \n",
    "TGT_VOCAB_SIZE = 1000 \n",
    "\n",
    "EMBED_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "MAX_LEN = 512\n",
    "FF_HIDDEN_MULT = 4\n",
    "DROPOUT = 0.1\n",
    "\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "PRETRAIN_EPOCHS = 5\n",
    "REWARD_MODEL_EPOCHS = 3\n",
    "PPO_UPDATES = 3\n",
    "EPS_CLIP = 0.2\n",
    "GAMMA = 0.99\n",
    "MAX_SEQ_LEN = 50 \n",
    "\n",
    "DEVICE = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # Shape: [1, max_len, embed_size]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = self.dim // self.n_head\n",
    "\n",
    "        assert self.head_dim * self.n_head == self.dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.fc_q = nn.Linear(dim, dim)\n",
    "        self.fc_k = nn.Linear(dim, dim)\n",
    "        self.fc_v = nn.Linear(dim, dim)\n",
    "\n",
    "        self.fc_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.fc_q(q)  # [batch_size, seq_len, dim]\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.n_head)  # [batch_size, n_head, seq_len, head_dim]\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.n_head)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.n_head)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [b, h, s, s]\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)  # [b, h, s, s]\n",
    "\n",
    "        out = torch.matmul(attn, v)  # [b, h, s, head_dim]\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')  # [b, s, dim]\n",
    "\n",
    "        out = self.fc_out(out)  # [b, s, dim]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, n_head, ff_hidden_mult=4, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(dim=embed_size, n_head=n_head)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * ff_hidden_mult),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size * ff_hidden_mult, embed_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_out))\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, n_head, ff_hidden_mult=4, dropout=0.1):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(dim=embed_size, n_head=n_head)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.cross_attn = MultiHeadAttention(dim=embed_size, n_head=n_head)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * ff_hidden_mult),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size * ff_hidden_mult, embed_size)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        # in self attention the target mask is used, to ensure the model\n",
    "        # does not cheet and considers only valid positions and perceiding tokens\n",
    "        self_attn_out = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(self_attn_out))\n",
    "\n",
    "\n",
    "        # in cross attention we use source mask\n",
    "        # to ensure generated tokens align with input context\n",
    "        cross_attn_out = self.cross_attn(x, enc_out, enc_out, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_out))\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        embed_size=512,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        n_head=8,\n",
    "        max_len=512,\n",
    "        ff_hidden_mult=4,\n",
    "        dropout=0.1,\n",
    "        tokenizer_pad_token_id=0\n",
    "    ):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, embed_size, padding_idx=tokenizer_pad_token_id)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab_size, embed_size, padding_idx=tokenizer_pad_token_id)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size, max_len)\n",
    "        self.pos_decoder = PositionalEncoding(embed_size, max_len)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_size, n_head, ff_hidden_mult, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(embed_size, n_head, ff_hidden_mult, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.tokenizer_pad_token_id = tokenizer_pad_token_id\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.tokenizer_pad_token_id).unsqueeze(1).unsqueeze(2) \n",
    "        return src_mask \n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_mask = (tgt != self.tokenizer_pad_token_id).unsqueeze(1).unsqueeze(2)  \n",
    "\n",
    "        subsequent_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device=tgt.device)).bool()  \n",
    "        subsequent_mask = subsequent_mask.unsqueeze(0).unsqueeze(0)  \n",
    "\n",
    "        tgt_mask = tgt_mask & subsequent_mask  \n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)  \n",
    "        tgt_mask = self.make_tgt_mask(tgt)  \n",
    "\n",
    "        enc_out = self.src_embed(src)  \n",
    "        enc_out = self.pos_encoder(enc_out)\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_out = layer(enc_out, src_mask)\n",
    "        dec_out = self.tgt_embed(tgt)  \n",
    "        dec_out = self.pos_decoder(dec_out)\n",
    "     \n",
    "        for layer in self.decoder_layers:\n",
    "            dec_out = layer(dec_out, enc_out, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc_out(dec_out)  \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, src_seq_len=20, tgt_seq_len=20, src_vocab_size=SRC_VOCAB_SIZE, tgt_vocab_size=TGT_VOCAB_SIZE, pad_idx=0):\n",
    "        super(ExampleDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.src_seq_len = src_seq_len\n",
    "        self.tgt_seq_len = tgt_seq_len\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        self.src_data = torch.randint(1, src_vocab_size, (num_samples, src_seq_len))\n",
    "        self.tgt_data = torch.randint(1, tgt_vocab_size, (num_samples, tgt_seq_len))\n",
    "        for i in range(num_samples):\n",
    "            pad_length = torch.randint(5, src_seq_len, (1,)).item()\n",
    "            self.src_data[i, pad_length:] = pad_idx\n",
    "            pad_length = torch.randint(5, tgt_seq_len, (1,)).item()\n",
    "            self.tgt_data[i, pad_length:] = pad_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_data[idx], self.tgt_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ExampleDataset(num_samples = 1000)\n",
    "sample_x, sample_y = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20]), torch.Size([20]))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.shape, sample_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, dataloader, n_epochs = 1, lr = 1e-4, voc_size = TGT_VOCAB_SIZE):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "    device = DEVICE\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), total = n_epochs, desc = 'Pretraining'):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(src, tgt_input)\n",
    "            loss = criterion(logits.reshape(-1, voc_size), tgt_output.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Pretraining Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(model, src, max_length=MAX_SEQ_LEN):\n",
    "\n",
    "    model.eval()\n",
    "    device = DEVICE\n",
    "    src = src.to(device)\n",
    "    src_mask = model.make_src_mask(src)\n",
    "    enc_out = model.src_embed(src)\n",
    "    enc_out = model.pos_encoder(enc_out)\n",
    "    for layer in model.encoder_layers:\n",
    "        enc_out = layer(enc_out, src_mask)\n",
    "\n",
    "    generated = torch.zeros(src.size(0), 1, dtype=torch.long).to(device)  \n",
    "    for _ in range(max_length):\n",
    "        dec_out = model.tgt_embed(generated)\n",
    "        dec_out = model.pos_decoder(dec_out)\n",
    "        for layer in model.decoder_layers:\n",
    "            dec_out = layer(dec_out, enc_out, src_mask, model.make_tgt_mask(generated))\n",
    "        logits = model.fc_out(dec_out) \n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  \n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0021, -0.0887, -0.0128, -0.0071,  0.0398, -0.0389, -0.0871, -0.1925,\n",
       "        -0.0797, -0.2083, -0.3549, -0.4410, -0.3121, -0.3292,  0.0134,  0.0187,\n",
       "        -0.0349, -0.2636, -0.1654, -0.1040, -0.2639, -0.0635, -0.1438, -0.1376,\n",
       "        -0.1905,  0.0026, -0.0844, -0.1132, -0.2061, -0.0153,  0.0542, -0.2214],\n",
       "       device='mps:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 voc_size,\n",
    "                 emb_size,\n",
    "                 hidden_size,\n",
    "                 n_layers,\n",
    "                 pad_token_id = 0):\n",
    "        super(RewardModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(voc_size, emb_size, padding_idx = pad_token_id)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = emb_size, nhead = 8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = n_layers)\n",
    "        self.fc_out = nn.Linear(emb_size, 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        emb = self.embedding(seq)\n",
    "        emb = emb.transpose(0, 1)\n",
    "        output = self.transformer_encoder(emb)\n",
    "        output = output.mean(dim = 0)\n",
    "        logits = self.fc_out(output)\n",
    "        return logits.squeeze()\n",
    "    \n",
    "\n",
    "\n",
    "rm = RewardModel(voc_size = TGT_VOCAB_SIZE, emb_size = EMBED_SIZE,\n",
    "                 hidden_size = HIDDEN_SIZE, n_layers = N_LAYERS, \n",
    "                 pad_token_id = 0).to(DEVICE)\n",
    "\n",
    "rm.forward(seq = torch.randint(0, TGT_VOCAB_SIZE, (BATCH_SIZE, MAX_SEQ_LEN)).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_human_feedback(seq):\n",
    "    reward = (seq == 42).float().mean().item()\n",
    "    return reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_reward_model_data(model, dataloader):\n",
    "    device = DEVICE\n",
    "    model.eval()\n",
    "    sequences = []\n",
    "    rewards = []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc = 'Collecting data for reward model'):\n",
    "            src = src.to(device)\n",
    "            generated_seq = generate_sequence(model, src, max_length = MAX_SEQ_LEN)\n",
    "            for seq in generated_seq:\n",
    "                reward = simulate_human_feedback(seq)\n",
    "                sequences.append(seq)\n",
    "                rewards.append(reward)\n",
    "    return sequences, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(model, src, max_length=MAX_SEQ_LEN):\n",
    "\n",
    "    model.eval()\n",
    "    device = DEVICE\n",
    "    src = src.to(device)\n",
    "    src_mask = model.make_src_mask(src)\n",
    "    enc_out = model.src_embed(src)\n",
    "    enc_out = model.pos_encoder(enc_out)\n",
    "    for layer in model.encoder_layers:\n",
    "        enc_out = layer(enc_out, src_mask)\n",
    "\n",
    "    generated = torch.zeros(src.size(0), 1, dtype=torch.long).to(device)  \n",
    "    for _ in range(max_length):\n",
    "        dec_out = model.tgt_embed(generated)\n",
    "        dec_out = model.pos_decoder(dec_out)\n",
    "        for layer in model.decoder_layers:\n",
    "            dec_out = layer(dec_out, enc_out, src_mask, model.make_src_mask(generated))\n",
    "        logits = model.fc_out(dec_out) \n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  \n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward_model(reward_model, sequences, rewards, num_epochs=2, lr=1e-4, b_size=32):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(reward_model.parameters(), lr=lr)\n",
    "    reward_model.train()\n",
    "    device = DEVICE\n",
    "    dataset = torch.utils.data.TensorDataset(torch.stack(sequences), torch.tensor(rewards))\n",
    "    dataloader = DataLoader(dataset, batch_size=b_size, shuffle=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for seq_batch, reward_batch in tqdm(dataloader, desc=f\"Training Reward Model Epoch {epoch+1}/{num_epochs}\"):\n",
    "            \n",
    "            seq_batch = seq_batch.to(device) \n",
    "            reward_batch = reward_batch.to(device, dtype = torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "            predicted_rewards = reward_model(seq_batch).float()  # [b]\n",
    "            loss = criterion(predicted_rewards, reward_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Reward Model Training Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, policy_model, reward_model,\n",
    "                 lr=1e-4, eps_clip=0.2,\n",
    "                 gamma=0.99, k_epochs=3,\n",
    "                 pad_token_id=0, voc_size=TGT_VOCAB_SIZE):\n",
    "        self.policy = policy_model\n",
    "        self.policy_old = TransformerSeq2Seq(\n",
    "            src_vocab_size=SRC_VOCAB_SIZE,\n",
    "            tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "            embed_size=EMBED_SIZE,\n",
    "            num_encoder_layers=N_LAYERS,\n",
    "            num_decoder_layers=N_LAYERS,\n",
    "            n_head=N_HEADS,\n",
    "            max_len=MAX_LEN,\n",
    "            ff_hidden_mult=FF_HIDDEN_MULT,\n",
    "            dropout=DROPOUT,\n",
    "            tokenizer_pad_token_id=pad_token_id\n",
    "        ).to(DEVICE)\n",
    "        self.policy_old.load_state_dict(policy_model.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.eps_clip = eps_clip\n",
    "        self.gamma = gamma\n",
    "        self.k_epochs = k_epochs\n",
    "        self.reward_model = reward_model\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.voc_size = voc_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def generate(self, src, max_len=MAX_SEQ_LEN):\n",
    "        self.policy.eval()\n",
    "        device = DEVICE\n",
    "        src = src.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_seq = generate_sequence(self.policy, src, max_length=max_len)\n",
    "        return generated_seq  \n",
    "\n",
    "    def update(self, memory):\n",
    "        old_srcs = torch.cat(memory.srcs, dim=0).to(DEVICE)       \n",
    "        old_states = torch.cat(memory.states, dim=0).to(DEVICE)     \n",
    "        old_actions = torch.cat(memory.actions, dim=0).to(DEVICE)   \n",
    "        old_logprobs = torch.cat(memory.logprobs, dim=0).to(DEVICE)  \n",
    "        rewards = torch.tensor(memory.rewards, dtype=torch.float32).to(DEVICE)  \n",
    "\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        advantages = rewards \n",
    "\n",
    "        for epoch in range(self.k_epochs):  # Iterate through epochs\n",
    "            logits = self.policy(old_srcs, old_states)  \n",
    "            logprobs = nn.functional.log_softmax(logits, dim=-1) \n",
    "            action_logprobs = logprobs.gather(2, old_actions.unsqueeze(-1)).squeeze(-1) \n",
    "\n",
    "            action_logprobs = action_logprobs.view(-1)  \n",
    "            old_logprobs = old_logprobs.view(-1)        \n",
    "            ratios = torch.exp(action_logprobs - old_logprobs.detach())  \n",
    "\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.k_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self):\n",
    "        self.states = []      \n",
    "        self.actions = []    \n",
    "        self.logprobs = []    \n",
    "        self.rewards = []    \n",
    "        self.srcs = []      \n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.srcs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finetune_llm_PPO(agent, dataloader, num_updates=1):\n",
    "    memory = PPOMemory()\n",
    "    for update in range(num_updates):\n",
    "        for src, _ in tqdm(dataloader, desc=f\"PPO Update {update+1}/{num_updates}\"):\n",
    "            src = src.to(DEVICE)\n",
    "            generated_seq = agent.generate(src)  \n",
    "\n",
    "            predicted_rewards = agent.reward_model(generated_seq)  \n",
    "\n",
    "            memory.srcs.append(src.cpu())\n",
    "\n",
    "            states = generated_seq[:, :-1] \n",
    "            actions = generated_seq[:, 1:]  \n",
    "            memory.states.append(states.cpu())\n",
    "            memory.actions.append(actions.cpu())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = agent.policy_old(src, states.to(DEVICE))  \n",
    "                logprobs = nn.functional.log_softmax(logits, dim=-1) \n",
    "                action_logprobs = logprobs.gather(2, actions.unsqueeze(-1)).squeeze(-1) \n",
    "                memory.logprobs.append(action_logprobs.cpu())\n",
    "\n",
    "\n",
    "            batch_rewards = predicted_rewards.detach().cpu().numpy() \n",
    "            seq_len = actions.size(1)\n",
    "            for reward in batch_rewards:\n",
    "                memory.rewards.extend([reward] * seq_len)\n",
    "\n",
    "        agent.update(memory)\n",
    "\n",
    "        memory.clear_memory()\n",
    "        print(f\"PPO Update {update+1}/{num_updates} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why updating old policy with new self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "\n",
    "# and also why [reward] * seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SEQ2SEQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretraining: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Epoch 1/1, Loss: 7.0815\n",
      "TRAINING REWARD MODEL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting data for reward model: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLECTING DATA FOR REWARD MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Reward Model Epoch 1/3: 100%|██████████| 4/4 [00:00<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Model Training Epoch 1/3, Loss: 7.1017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Reward Model Epoch 2/3: 100%|██████████| 4/4 [00:00<00:00, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Model Training Epoch 2/3, Loss: 1.8748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Reward Model Epoch 3/3: 100%|██████████| 4/4 [00:00<00:00, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Model Training Epoch 3/3, Loss: 0.3821\n"
     ]
    }
   ],
   "source": [
    "dataset = ExampleDataset(num_samples = 100)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "\n",
    "model = TransformerSeq2Seq(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_encoder_layers=N_LAYERS,\n",
    "    num_decoder_layers=N_LAYERS,\n",
    "    n_head=N_HEADS,\n",
    "    max_len=MAX_LEN,\n",
    "    ff_hidden_mult=FF_HIDDEN_MULT,\n",
    "    dropout=DROPOUT,\n",
    "    tokenizer_pad_token_id=0\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "reward_model = RewardModel(voc_size = TGT_VOCAB_SIZE,\n",
    "                           emb_size = EMBED_SIZE,\n",
    "                           hidden_size = HIDDEN_SIZE,\n",
    "                           n_layers = N_LAYERS,\n",
    "                           pad_token_id = 0).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('TRAINING SEQ2SEQ')\n",
    "train_seq2seq(model, dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('TRAINING REWARD MODEL...')\n",
    "\n",
    "sequences, rewards = collect_reward_model_data(model, dataloader)\n",
    "print('COLLECTING DATA FOR REWARD MODEL')\n",
    "\n",
    "\n",
    "train_reward_model(reward_model, sequences, rewards, num_epochs = REWARD_MODEL_EPOCHS,\n",
    "                   lr = LR, b_size = BATCH_SIZE)\n",
    "\n",
    "ppo_agent = PPOAgent(\n",
    "    policy_model = model,\n",
    "    reward_model = reward_model,\n",
    "    lr = LR,\n",
    "    eps_clip = EPS_CLIP,\n",
    "    gamma = GAMMA,\n",
    "    k_epochs = 4,\n",
    "    pad_token_id = 0,\n",
    "    voc_size = TGT_VOCAB_SIZE\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
